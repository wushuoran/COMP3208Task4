{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "742db20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user max id: 138493, item max id: 26744\n",
      "trainset:epoch 0 | MAE 1.1685\n",
      "testset: epoch 0 | MAE 1.1818\n",
      "trainset:epoch 1 | MAE 0.8815\n",
      "testset: epoch 1 | MAE 0.9014\n",
      "trainset:epoch 2 | MAE 0.8278\n",
      "testset: epoch 2 | MAE 0.8496\n",
      "trainset:epoch 3 | MAE 0.8059\n",
      "testset: epoch 3 | MAE 0.8302\n",
      "trainset:epoch 4 | MAE 0.7889\n",
      "testset: epoch 4 | MAE 0.8162\n",
      "trainset:epoch 5 | MAE 0.7725\n",
      "testset: epoch 5 | MAE 0.8029\n",
      "trainset:epoch 6 | MAE 0.7587\n",
      "testset: epoch 6 | MAE 0.7916\n",
      "trainset:epoch 7 | MAE 0.7459\n",
      "testset: epoch 7 | MAE 0.7815\n",
      "trainset:epoch 8 | MAE 0.7345\n",
      "testset: epoch 8 | MAE 0.7721\n",
      "trainset:epoch 9 | MAE 0.7246\n",
      "testset: epoch 9 | MAE 0.7645\n",
      "trainset:epoch 10 | MAE 0.7159\n",
      "testset: epoch 10 | MAE 0.7575\n",
      "trainset:epoch 11 | MAE 0.7076\n",
      "testset: epoch 11 | MAE 0.7506\n",
      "trainset:epoch 12 | MAE 0.7005\n",
      "testset: epoch 12 | MAE 0.7448\n",
      "trainset:epoch 13 | MAE 0.6942\n",
      "testset: epoch 13 | MAE 0.7396\n",
      "trainset:epoch 14 | MAE 0.6884\n",
      "testset: epoch 14 | MAE 0.7350\n",
      "trainset:epoch 15 | MAE 0.6832\n",
      "testset: epoch 15 | MAE 0.7306\n",
      "trainset:epoch 16 | MAE 0.6783\n",
      "testset: epoch 16 | MAE 0.7266\n",
      "trainset:epoch 17 | MAE 0.6734\n",
      "testset: epoch 17 | MAE 0.7222\n",
      "trainset:epoch 18 | MAE 0.6697\n",
      "testset: epoch 18 | MAE 0.7190\n",
      "trainset:epoch 19 | MAE 0.6665\n",
      "testset: epoch 19 | MAE 0.7164\n",
      "trainset:epoch 20 | MAE 0.6624\n",
      "testset: epoch 20 | MAE 0.7130\n",
      "trainset:epoch 21 | MAE 0.6592\n",
      "testset: epoch 21 | MAE 0.7097\n",
      "trainset:epoch 22 | MAE 0.6571\n",
      "testset: epoch 22 | MAE 0.7078\n",
      "trainset:epoch 23 | MAE 0.6550\n",
      "testset: epoch 23 | MAE 0.7061\n",
      "trainset:epoch 24 | MAE 0.6524\n",
      "testset: epoch 24 | MAE 0.7037\n",
      "trainset:epoch 25 | MAE 0.6504\n",
      "testset: epoch 25 | MAE 0.7017\n",
      "trainset:epoch 26 | MAE 0.6485\n",
      "testset: epoch 26 | MAE 0.6997\n",
      "trainset:epoch 27 | MAE 0.6468\n",
      "testset: epoch 27 | MAE 0.6979\n",
      "trainset:epoch 28 | MAE 0.6450\n",
      "testset: epoch 28 | MAE 0.6964\n",
      "trainset:epoch 29 | MAE 0.6434\n",
      "testset: epoch 29 | MAE 0.6947\n",
      "trainset:epoch 30 | MAE 0.6424\n",
      "testset: epoch 30 | MAE 0.6937\n",
      "trainset:epoch 31 | MAE 0.6409\n",
      "testset: epoch 31 | MAE 0.6922\n",
      "trainset:epoch 32 | MAE 0.6397\n",
      "testset: epoch 32 | MAE 0.6908\n",
      "trainset:epoch 33 | MAE 0.6390\n",
      "testset: epoch 33 | MAE 0.6902\n",
      "trainset:epoch 34 | MAE 0.6376\n",
      "testset: epoch 34 | MAE 0.6886\n",
      "trainset:epoch 35 | MAE 0.6365\n",
      "testset: epoch 35 | MAE 0.6876\n",
      "trainset:epoch 36 | MAE 0.6365\n",
      "testset: epoch 36 | MAE 0.6871\n",
      "trainset:epoch 37 | MAE 0.6357\n",
      "testset: epoch 37 | MAE 0.6861\n",
      "trainset:epoch 38 | MAE 0.6353\n",
      "testset: epoch 38 | MAE 0.6854\n",
      "trainset:epoch 39 | MAE 0.6345\n",
      "testset: epoch 39 | MAE 0.6846\n",
      "Train_Data:epoch 0 | MAE 1.0990\n",
      "Train_Data:epoch 1 | MAE 0.8622\n",
      "Train_Data:epoch 2 | MAE 0.8205\n",
      "Train_Data:epoch 3 | MAE 0.7980\n",
      "Train_Data:epoch 4 | MAE 0.7803\n",
      "Train_Data:epoch 5 | MAE 0.7641\n",
      "Train_Data:epoch 6 | MAE 0.7499\n",
      "Train_Data:epoch 7 | MAE 0.7380\n",
      "Train_Data:epoch 8 | MAE 0.7269\n",
      "Train_Data:epoch 9 | MAE 0.7178\n",
      "Train_Data:epoch 10 | MAE 0.7086\n",
      "Train_Data:epoch 11 | MAE 0.7015\n",
      "Train_Data:epoch 12 | MAE 0.6952\n",
      "Train_Data:epoch 13 | MAE 0.6887\n",
      "Train_Data:epoch 14 | MAE 0.6828\n",
      "Train_Data:epoch 15 | MAE 0.6780\n",
      "Train_Data:epoch 16 | MAE 0.6736\n",
      "Train_Data:epoch 17 | MAE 0.6696\n",
      "Train_Data:epoch 18 | MAE 0.6659\n",
      "Train_Data:epoch 19 | MAE 0.6630\n",
      "Train_Data:epoch 20 | MAE 0.6600\n",
      "Train_Data:epoch 21 | MAE 0.6568\n",
      "Train_Data:epoch 22 | MAE 0.6546\n",
      "Train_Data:epoch 23 | MAE 0.6521\n",
      "Train_Data:epoch 24 | MAE 0.6508\n",
      "Train_Data:epoch 25 | MAE 0.6489\n",
      "Train_Data:epoch 26 | MAE 0.6478\n",
      "Train_Data:epoch 27 | MAE 0.6457\n",
      "Train_Data:epoch 28 | MAE 0.6445\n",
      "Train_Data:epoch 29 | MAE 0.6431\n",
      "Train_Data:epoch 30 | MAE 0.6419\n",
      "Train_Data:epoch 31 | MAE 0.6416\n",
      "Train_Data:epoch 32 | MAE 0.6407\n",
      "Train_Data:epoch 33 | MAE 0.6404\n",
      "Train_Data:epoch 34 | MAE 0.6398\n",
      "Train_Data:epoch 35 | MAE 0.6389\n",
      "Train_Data:epoch 36 | MAE 0.6384\n",
      "Train_Data:epoch 37 | MAE 0.6377\n",
      "Train_Data:epoch 38 | MAE 0.6370\n",
      "Train_Data:epoch 39 | MAE 0.6363\n",
      "successfully predict！\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import functools\n",
    "\n",
    "# read csv\n",
    "def load_csv(filepath,test=False):\n",
    "    with open(filepath) as f:\n",
    "        reader = csv.reader(f)\n",
    "        if not test:\n",
    "           rows = [[int(x[0]), int(x[1]), float(x[2])] for x in reader]\n",
    "        else:\n",
    "            rows = [[int(x[0]), int(x[1]), int(x[2])] for x in reader]\n",
    "\n",
    "    return rows\n",
    "\n",
    "\n",
    "# split the provided data into training set 9 : validation set 1\n",
    "def SplitTrainData(Train_data):\n",
    "    data_num=Train_data.shape[0]\n",
    "    np.random.shuffle(Train_data)\n",
    "    trainset=list(Train_data[:data_num//10*9])         #81513\n",
    "    testset=list(Train_data[data_num//10*9:])          #9057\n",
    "\n",
    "    # re-order the trainset and testset according to uesrid and itemid (raising order)\n",
    "    def cmp(x, y):\n",
    "        if x[0] != y[0]:\n",
    "            return x[0] - y[0]\n",
    "        else:\n",
    "            return x[1] - y[1]\n",
    "    trainset.sort(key = functools.cmp_to_key(cmp))\n",
    "    testset.sort(key = functools.cmp_to_key(cmp))\n",
    "    trainset=[[int(u),int(i),float(r)] for u, i, r in trainset]\n",
    "    testset = [[int(u), int(i), float(r)] for u, i, r in testset]\n",
    "\n",
    "    f = open('trainset.csv', 'w',newline='')\n",
    "    writer = csv.writer(f)\n",
    "    for i in trainset:\n",
    "        writer.writerow(i)\n",
    "    f.close()\n",
    "\n",
    "    f = open('testset.csv', 'w',newline='')\n",
    "    writer = csv.writer(f)\n",
    "    for i in testset:\n",
    "        writer.writerow(i)\n",
    "    f.close()\n",
    "    return trainset,testset\n",
    "\n",
    "# sum up max and min value of users and items in data\n",
    "def data_statistics(data):\n",
    "    max_u = 0\n",
    "    max_i = 0\n",
    "    min_u = 10000\n",
    "    min_i = 10000\n",
    "    for triple in data:\n",
    "        u, i = triple[0], triple[1]\n",
    "        if u >= max_u:\n",
    "            max_u = u\n",
    "\n",
    "        if i >= max_i:\n",
    "            max_i = i\n",
    "\n",
    "        if u<=min_u:\n",
    "            min_u=u\n",
    "\n",
    "        if i<=min_i:\n",
    "            min_i=i\n",
    "    return max_u + 1, max_i + 1   # indices of users and items start from 0,therefore +1 to use in matrix\n",
    "\n",
    "# calculate MAE\n",
    "def MAE(r, predict_ratings):\n",
    "    return np.mean(np.absolute(np.array((r) - np.array(predict_ratings))))\n",
    "\n",
    "# same as final_predic_ratings, but return the MAE\n",
    "def evalueMAE(testset, lfm):\n",
    "    testset = np.array(testset)\n",
    "    u = testset[:,0].astype(int)\n",
    "    i = testset[:,1].astype(int)\n",
    "    r = testset[:,2]\n",
    "    predict_ratings = lfm.forward(u, i)\n",
    "    predict_ratings = predict_ratings.reshape(-1)\n",
    "    # round up the ratings\n",
    "    data=np.array(predict_ratings)\n",
    "    for i in range(len(data)):\n",
    "        if data[i] < 1.25:\n",
    "            data[i] = 1.0\n",
    "        elif data[i] >= 1.25 and data[i] < 1.75:\n",
    "            data[i] = 1.5\n",
    "        elif data[i] >= 1.75 and data[i] < 2.25:\n",
    "            data[i] = 2.0\n",
    "        elif data[i] >= 2.25 and data[i] < 2.75:\n",
    "            data[i] = 2.5\n",
    "        elif data[i] >= 2.75 and data[i] < 3.25:\n",
    "            data[i] = 3.0\n",
    "        elif data[i] >= 3.25 and data[i] < 3.75:\n",
    "            data[i] = 3.5\n",
    "        elif data[i] >= 3.75 and data[i] < 4.25:\n",
    "            data[i] = 4.0\n",
    "        elif data[i] >= 4.25 and data[i] < 4.75:\n",
    "            data[i] =4.5\n",
    "        elif data[i] >= 4.75:\n",
    "            data[i] = 5.0\n",
    "    predict_ratings=data\n",
    "    return MAE(r, predict_ratings)\n",
    "\n",
    "# same as evalueMAE, return prediction array (can modify)\n",
    "def final_predic_ratings(testset, lfm):\n",
    "    testset = np.array(testset)\n",
    "    u = testset[:,0].astype(int)\n",
    "    i = testset[:,1].astype(int)\n",
    "    r = testset[:,2]\n",
    "    predict_ratings = lfm.forward(u, i)\n",
    "    predict_ratings = predict_ratings.reshape(-1)\n",
    "    data=np.array(predict_ratings)\n",
    "    for i in range(len(data)):\n",
    "        if data[i] < 1.25:\n",
    "            data[i] = 1.0\n",
    "        elif data[i] >= 1.25 and data[i] < 1.75:\n",
    "            data[i] = 1.5\n",
    "        elif data[i] >= 1.75 and data[i] < 2.25:\n",
    "            data[i] = 2.0\n",
    "        elif data[i] >= 2.25 and data[i] < 2.75:\n",
    "            data[i] = 2.5\n",
    "        elif data[i] >= 2.75 and data[i] < 3.25:\n",
    "            data[i] = 3.0\n",
    "        elif data[i] >= 3.25 and data[i] < 3.75:\n",
    "            data[i] = 3.5\n",
    "        elif data[i] >= 3.75 and data[i] < 4.25:\n",
    "            data[i] = 4.0\n",
    "        elif data[i] >= 4.25 and data[i] < 4.75:\n",
    "            data[i] =4.5\n",
    "        elif data[i] >= 4.75:\n",
    "            data[i] = 5.0\n",
    "    predict_ratings=data\n",
    "    return predict_ratings\n",
    "\n",
    "class LFM():\n",
    "    def __init__(self, max_u, max_i, dim):\n",
    "        self.p = np.random.uniform(size = (max_u, dim))   #  randomly generate user matrix, demension is max_u * dim\n",
    "        self.q = np.random.uniform(size = (max_i, dim))   #  randomly generate item matrix, demension is max_i * dim\n",
    "        self.bu = np.random.uniform(size = (max_u, 1))    #  randomly generate user's bias term\n",
    "        self.bi = np.random.uniform(size = (max_i, 1))    #  randomly generate item's bias term\n",
    "\n",
    "    # forward propagation\n",
    "    def forward(self, u, i):\n",
    "        return np.sum(self.p[u] * self.q[i], axis=1, keepdims=True) + self.bu[u] + self.bi[i]     # predict ratings\n",
    "\n",
    "    # back propagation, iterate model parameters according to gradient descend method\n",
    "    def backward(self, r, predict_ratings, u, i, lr, lamda): # r = actual rating\n",
    "        # loss function should be the square of difference of actual and predict ratings\n",
    "        # because of gradient descend, we need to compute the partial derivative of loss function\n",
    "        # for convience, compute the difference directly\n",
    "        loss = r - predict_ratings        \n",
    "        # update gradient\n",
    "        self.p[u] += lr * (loss * self.q[i] - lamda * self.p[u])\n",
    "        self.q[i] += lr * (loss * self.p[u] - lamda * self.q[i])\n",
    "        self.bu[u] += lr * (loss * lamda * self.bu[u])\n",
    "        self.bi[i] += lr * (loss * lamda * self.bi[i])\n",
    "\n",
    "# data iterator\n",
    "class DataIter():\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = np.array(dataset)\n",
    "\n",
    "    # each iteration return batch_size data\n",
    "    def iter(self, batch_size):\n",
    "        for _ in range(len(self.dataset)//batch_size):\n",
    "            np.random.shuffle(self.dataset)    #  shuffle data\n",
    "            yield self.dataset[:batch_size]    # output, batch_size as unit\n",
    "\n",
    "\n",
    "def train(trainset,testset, max_u, max_i):\n",
    "    epoch = 40        #  epoch = 20  number of iterations\n",
    "    batchSize = 1024000     #  batchSize = 1024\n",
    "    lr = 0.01            #  lr = 0.01 learning rate 0.01\n",
    "    lamda = 0.1          #  lamda = 0.1 \n",
    "    factor_dim = 64      #  factor_dim = 64, p matrix is max_u * factor_dim, q matrix is (max_i * factor_dim).T\n",
    "\n",
    "    # initialize LFM model\n",
    "    lfm = LFM(max_u, max_i, factor_dim)\n",
    "    # initialize data iterator\n",
    "    dataIter = DataIter(trainset)\n",
    "\n",
    "    for e in range(epoch):\n",
    "        for batch in (dataIter.iter(batchSize)):\n",
    "            u = batch[:,0].astype(int)\n",
    "            i = batch[:,1].astype(int)\n",
    "            r = batch[:,2].reshape(-1, 1)   \n",
    "\n",
    "            # get predict ratings\n",
    "            predict_ratings = lfm.forward(u, i)\n",
    "            # gradient descent iteration\n",
    "            lfm.backward(r, predict_ratings, u, i, lr, lamda)\n",
    "\n",
    "        print('trainset:epoch {} | MAE {:.4f}'.format(e, evalueMAE(trainset, lfm)))\n",
    "        print('testset: epoch {} | MAE {:.4f}'.format(e, evalueMAE(testset, lfm)))\n",
    "\n",
    "def final_train(Train_data, Test_data, max_u, max_i):\n",
    "    epoch = 40    \n",
    "    batchSize = 1024000\n",
    "    lr = 0.01            \n",
    "    lamda = 0.1          \n",
    "    factor_dim = 64    \n",
    "\n",
    "    lfm = LFM(max_u, max_i, factor_dim)\n",
    "    dataIter = DataIter(Train_data)\n",
    "\n",
    "    for e in range(epoch):\n",
    "        for batch in (dataIter.iter(batchSize)):\n",
    "            u = batch[:,0].astype(int)\n",
    "            i = batch[:,1].astype(int)\n",
    "            r = batch[:,2].reshape(-1, 1)      \n",
    "\n",
    "            predict_ratings = lfm.forward(u, i)\n",
    "            lfm.backward(r, predict_ratings, u, i, lr, lamda)\n",
    "\n",
    "        print('Train_Data:epoch {} | MAE {:.4f}'.format(e, evalueMAE(Train_data, lfm)))\n",
    "        if e == epoch - 1:\n",
    "            # print('Test_Data:epoch {} | MAE {:.4f}'.format(e, evalueMAE(Test_data, lfm)))\n",
    "            data = np.array(final_predic_ratings(Test_data, lfm))\n",
    "            results = []\n",
    "            for i in range(len(Test_data)):\n",
    "                results.append([Test_data[i][0], Test_data[i][1], data[i], Test_data[i][2]])\n",
    "\n",
    "            f = open('results.csv', 'w', newline='')\n",
    "            writer = csv.writer(f)\n",
    "            for i in results:\n",
    "                writer.writerow(i)\n",
    "            f.close()\n",
    "            print('successfully predict！')\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ##### data processing\n",
    "    Train_data = load_csv('comp3208_20m_train_withratings.csv')\n",
    "    Test_data = load_csv('comp3208_20m_test_withoutratings.csv',test=True)\n",
    "\n",
    "    trainset, testset=SplitTrainData(np.array(Train_data))\n",
    "    trainset=load_csv('trainset.csv')\n",
    "    testset=load_csv('testset.csv')\n",
    "\n",
    "    train_max_u, train_max_i = data_statistics(Train_data)\n",
    "    test_max_u, test_max_i = data_statistics(Test_data)\n",
    "\n",
    "    max_u = train_max_u if train_max_u >= test_max_u else test_max_u\n",
    "    max_i = train_max_i if train_max_i >= test_max_i else test_max_i\n",
    "    \n",
    "    print('user max id: {}, item max id: {}'.format(max_u - 1, max_i - 1 ))  \n",
    "\n",
    "    train(trainset, testset, max_u, max_i)\n",
    "\n",
    "    final_train(Train_data, Test_data, max_u, max_i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5663c837",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0c7efe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
